{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3455097",
   "metadata": {},
   "source": [
    "<h1><center>Project</center></h1>\n",
    "<h2><center>Micro-452 : Basics of Mobile Robotics</center></h2>\n",
    "<h3><center>Group 11</center></h3>\n",
    "<h3><center>Benoist Marguerite, Chang Chun-Tzu, Maradan Théodore, Tambourin Noé</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8fc8b",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This report presents the project done in the Basics of Mobile Robotics class. The project consists in programming a robot (the \"Thymio\") to navigate in an environment containing a set of (expected and unexpected) obstacles.\n",
    "\n",
    "To do so, the robot should first be able to build a global map of its environment using a camera (<b>vision</b>). From the map, the robot should determine the shortest path from its starting point to its goal (<b>global naviagation</b>). Then, <b>filtering</b> the information coming from the camera and from the built-in sensors the robot should travel to a goal point set anywhere on the map using <b>motor control</b>. Finally, the robot should avoid any unexpected obstacle on its path using <b>local navigation</b>.\n",
    "\n",
    "The global structure of the project with the main fluxes of information between the different parts of it can be seen in Figure 1.\n",
    "\n",
    "<img src=\"Report_Figures/Global_Diagram.png\" style='text-align: center' width=\"550\" height=\"208;\"/><p style='text-align: center;'><b>Figure 1: Global Diagram of the Project</b></p>\n",
    "\n",
    "The sections below describe in detail our solution to the problem and the choices and assumptions made through the realization of this project.\n",
    "\n",
    "In particular, section 2 describes the implementation of the diagram above using a state machine as well as the environment chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c627765",
   "metadata": {},
   "source": [
    "## 2. Choices & Assumptions\n",
    "\n",
    "In order to combine the different parts of the project, we decided to use a state machine using two states: Obstacle avoidance & global navigation. The former is used to get around unforeseen obstacles (obstacles that were not included in the global map) while the latter is used to follow the shortest path from the starting point to the goal point. An illustration of the state machine used is provided in Figure 2.\n",
    "\n",
    "<font color='red'>INCLUDE FIGURE STATE MACHINE</font>\n",
    "\n",
    "<img src=\"Report_Figures/State_Machine_Diagram.jpeg\" style='text-align: center' width=\"550\" height=\"208;\"/><p style='text-align: center;'><b>Figure 2: State Machine used for the Project</b></p>\n",
    "\n",
    "The environment constituting the global map was chosen to be blank with 2D black obstacles (obstacles without thickness). The contrast between the map and the obstacles allows for a better detection of the obstacles by the camera. In contrast, the unexpected obstacles are 3D (non-null thickness).\n",
    "\n",
    "This distinction ensures that the obstacles detected by the proximity sensors are unexpected obstacles and avoids confusion with obstacles being part of the global map.\n",
    "\n",
    "<font color='red'>INCLUDE FIGURE ENVIRONMENT</font>\n",
    "\n",
    "<img src=\"Report_Figures/Environment.png\" style='text-align: center' width=\"550\" height=\"208;\"/><p style='text-align: center;'><b>Figure 3: Environment</b></p>\n",
    "\n",
    "In order to identify the robot and the goal point, ArUco markers are used. These synthetic markers are easy to identify using an appropriate library (OpenCV). Moreover, they ensure a reliable robot pose acquisition by the camera.\n",
    "\n",
    "<font color='red'>INCLUDE FIGURE ARUCO MARKER</font>\n",
    "\n",
    "<img src=\"Report_Figures/ArUco_Marker.png\" style='text-align: center' width=\"550\" height=\"208;\"/><p style='text-align: center;'><b>Figure 4: ArUco Markers</b></p>\n",
    "\n",
    "Finally, the camera is fixed above the global map so that it sees the entire environment while avoiding as much as possible distortions if the map due to the camera not being parallel with the plane of the map.\n",
    "\n",
    "<font color='red'>INCLUDE FIGURE ENVIRONMENT + CAMERA</font>\n",
    "\n",
    "<img src=\"Report_Figures/Environment_Camera.png\" style='text-align: center' width=\"550\" height=\"208;\"/><p style='text-align: center;'><b>Figure 5: Environment with Camera</b></p>\n",
    "\n",
    "<font color='red'>INCLUDE SENTENCE CODE IS ONLY PART OF IT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b22a8b",
   "metadata": {},
   "source": [
    "## 3. Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b77eb",
   "metadata": {},
   "source": [
    "### 3.1 Global Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84315bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_globalmap(self, image, start_id=0, goal_id=1):\n",
    "        # Generate globalmap : 0:free 1:occupied\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Otsu's thresholding after Gaussian filtering\n",
    "        blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "        ret, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "        # Generate Global map\n",
    "        # from 0,255 convert to 0(free),1(occupied);change to (x,y) coordinate\n",
    "        temp_globalmap = np.transpose(np.array(thresh < 255, dtype=int))\n",
    "\n",
    "        # Detect start and goal by aruco marker\n",
    "        # Return start and goal position in (x,y) coordinate; if not detected return None\n",
    "        # start_id: start aruco marker id\n",
    "        # goal_id=: goal aruco marker id\n",
    "\n",
    "        # Define aruco marker detector\n",
    "        arucoDict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_4X4_50)\n",
    "        arucoParams = cv2.aruco.DetectorParameters_create()\n",
    "\n",
    "        # Detect aruco marker\n",
    "        (corners, ids, rejected) = cv2.aruco.detectMarkers(image, arucoDict, parameters=arucoParams)\n",
    "        start = None\n",
    "        goal = None\n",
    "        s_start = 50 #start margin\n",
    "        s_goal = 20 #goal margin\n",
    "        if ids is not None:\n",
    "            for corner, number in zip(corners, ids):\n",
    "                (topLeft, topRight, bottomRight, bottomLeft) = corner[0].astype(int)\n",
    "                centerpoint = ((topLeft[0] + bottomRight[0]) / 2, (topLeft[1] + bottomRight[1]) / 2)\n",
    "                side_length=math.dist(topLeft, topRight)/2\n",
    "                self.side_length=side_length\n",
    "                if number == start_id:\n",
    "                    start = centerpoint\n",
    "                    #get rid of aruco marker of start point\n",
    "                    print(\"start:\",start)\n",
    "                    x_small=int(centerpoint[0]-side_length)-s_start\n",
    "                    x_large=int(centerpoint[0]+side_length)+s_start\n",
    "                    y_small =int(centerpoint[1]-side_length)-s_start\n",
    "                    y_large=int(centerpoint[1]+side_length)+s_start\n",
    "                    if x_small<0: x_small=0\n",
    "                    if y_small<0:y_small=0\n",
    "                    if x_large>=width:x_large=width-1\n",
    "                    if y_large>=height:y_large=height-1\n",
    "                    temp_globalmap[x_small:x_large,y_small:y_large] = 0\n",
    "\n",
    "                elif number == goal_id:\n",
    "                    goal = centerpoint\n",
    "                    #get rid of aruco marker of goal point\n",
    "                    print(\"goal:\",goal)\n",
    "                    x_small = int(centerpoint[0] - side_length) - s_goal\n",
    "                    x_large = int(centerpoint[0] + side_length) + s_goal\n",
    "                    y_small = int(centerpoint[1] - side_length) - s_goal\n",
    "                    y_large = int(centerpoint[1] + side_length) + s_goal\n",
    "                    if x_small < 0: x_small = 0\n",
    "                    if y_small < 0: y_small = 0\n",
    "                    if x_large >= width: x_large = width - 1\n",
    "                    if y_large >= height: y_large = height - 1\n",
    "                    temp_globalmap[x_small:x_large, y_small:y_large] = 0\n",
    "                    \n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        globalmap = temp_globalmap.copy()\n",
    "        if start is None:\n",
    "            print(\"NOT detect start point\")\n",
    "        else:\n",
    "            self.start = start\n",
    "        if goal is None:\n",
    "            print(\"NOT detect goal point\")\n",
    "        else:\n",
    "            self.goal = goal\n",
    "\n",
    "        return globalmap\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd1415",
   "metadata": {},
   "source": [
    "The first step of the project is to construct the map with the vision. We fisrt use Otsu's threshold algorithm to recognized the black global obstacles and then use aruco marker to detect goal point and start point which is where robot is at the beginning.\n",
    "\n",
    "#### 3.1.1 Otsu's threshold method\n",
    "\n",
    "Image thresholding binarizes a grayscale image based on pixel values and a threshold value. For example, when the pixel value is samller than the threshold value, this pixel will be marked as black. When the pixel value is larger then the threshold value, this pixel will be marked as white. The threshold algorithm we choosed is Otsu's threshold. Comparing to simple threshold method which use an manually choosen value as threshold, Otsu's threshold method [1] tries to find a global optimal threshold value from image histogram which can minimize the weighted within-class variance given by:\n",
    "\n",
    "$\\sigma_w^2(t) = q_1(t)\\sigma_1^2(t)+q_2(t)\\sigma_2^2(t)\\$\n",
    "\n",
    "where\n",
    "\n",
    "$q_1(t) = \\sum_{i=1}^{t} P(i) \\quad \\& \\quad q_2(t) = \\sum_{i=t+1}^{I} P(i)$\n",
    "\n",
    "$\\mu_1(t) = \\sum_{i=1}^{t} \\frac{iP(i)}{q_1(t)} \\quad \\& \\quad \\mu_2(t) = \\sum_{i=t+1}^{I} \\frac{iP(i)}{q_2(t)}$\n",
    "\n",
    "$\\sigma_1^2(t) = \\sum_{i=1}^{t} [i-\\mu_1(t)]^2 \\frac{P(i)}{q_1(t)} \\quad \\& \\quad \\sigma_2^2(t) = \\sum_{i=t+1}^{I} [i-\\mu_2(t)]^2 \\frac{P(i)}{q_2(t)}$\n",
    "\n",
    "In our map, we have black (global obstacle) and white (free move space) pixels, so the image histogram will have two peaks. Then, Otsu's threshold method find a value lying between two peaks to minimizes both classes variances which is the optimal threshold value we used. By using Otsu's threshold, it will help us to choose the optimal threshold value rather than manually choose the threshold value evrytime depending on the environment when using simple threshold method. Following figures show the difference between using two methods with different environment,\n",
    "<table><tr>\n",
    "<td> <img src=\"Report_Figures/map1.png\" style='text-align: left' width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Map 1</b></p></td>\n",
    "<td><img src=\"Report_Figures/Otsus_thresholding_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Otsu's threshold</b></p></td>\n",
    "<td><img src=\"Report_Figures/simple_threshold_method_50_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Simple threshold, threshold=50</b></p></td>\n",
    "<td><img src=\"Report_Figures/simple_threshold_method_127_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Simple threshold, threshold=127</b></p></td>\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Report_Figures/map2.png\" style='text-align: left' width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Map 2</b></p></td>\n",
    "<td><img src=\"Report_Figures/Otsus_thresholding_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Otsu's threshold</b></p></td>\n",
    "<td><img src=\"Report_Figures/simple_threshold_method_50_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Simple threshold, threshold=50</b></p></td>\n",
    "<td><img src=\"Report_Figures/simple_threshold_method_127_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Simple threshold, threshold=127</b></p></td>\n",
    "</tr></table>\n",
    "<p style='text-align: center;'><b>Figure 6: Effect of different thresholds</b></p>\n",
    "\n",
    "\n",
    "We can observe that Otsu's threshold shows stable performance with differnt evironment. Simple threshold method performs better with threshold value 127 in Map 1, however, in Map 2, threshold value 50 perfomrs better. Therefore, using simple threshold method may need to manually adjust the threshold value when the environment changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88242af7",
   "metadata": {},
   "source": [
    "#### 3.1.2 ArUco Marker\n",
    "\n",
    "Then, we need to detect goal point and start point which is the robot position for constructing global map. To detect the position of objects, we use aruco marker which is fast and robust.\n",
    "\n",
    "Aruco marker [2] provides the position of its four conners and uses its inner binary matrix to determine its id. To detect an aruco marker, it fisrt apllies adaptive threshold and then extractes the contours of shapes in the image. If the extracted shape is not a convex or not simillar to a square shape, it will be discarded. After selecting the candidates of marker, it need to extracte the inner bits of each candidates. Otsu's thresholding  is used to first seperate black and white pixels in the image. Then, the extracted shape is divided to differnt cell according to the size of marker and boarder. The cell which contains more black pixels will be considered as a black bit, vice versa. Finally, we can get four coners and id by binary codification.\n",
    "\n",
    "Below shows some example of ArUco marker detection.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Report_Figures/arucomarker_map1.png\" style='text-align: left' width=\"320\" height=\"416;\"/><p style='text-align: center;'><b>Map 1</b></p></td>\n",
    "<td> <img src=\"Report_Figures/arucomarker_map2.png\" style='text-align: left' width=\"320\" height=\"416;\"/><p style='text-align: center;'><b>Map 2</b></p></td>\n",
    "</tr></table>\n",
    "<p style='text-align: center;'><b>Figure 7: Detection of ArUco Markers</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68654b31",
   "metadata": {},
   "source": [
    "#### 3.1.3 Add margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aec707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_margin(self,globalmap,robot_size=70):\n",
    "       #object inflation, add margin to global map\n",
    "       #robot_size: robot_size in mm\n",
    "       #return global map with margin\n",
    "\n",
    "        # Compute pixel to distance value\n",
    "        pixel_to_distance = self.compute_pixel_to_distance()\n",
    "\n",
    "        # obstacle inflation\n",
    "        # 3x3 structuring element with connectivity 2\n",
    "        struct = ndimage.generate_binary_structure(2, 2)\n",
    "        # robot_size:radius of robot(unit:mm)\n",
    "        margin = int(robot_size / pixel_to_distance) + 1  # margin at least need to be 1, can't be 0\n",
    "        globalmap = ndimage.binary_dilation(globalmap, structure=struct, iterations=margin).astype(globalmap.dtype)\n",
    "\n",
    "        self.globalmap = globalmap\n",
    "\n",
    "        return self.globalmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962480c9",
   "metadata": {},
   "source": [
    "Since the robot is not only a point, we need to add margin to the global obstacles to insure that every part of robot will not go into it. Here, we use binary dilation to increase the area of obstacles with the structure we defined. We defined the increasing margin is a 3x3 strcuting element with connectivity 2, which is:\n",
    " \\begin{array}{cc} \n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 1\\\\\n",
    "\\end{array}\n",
    "If the original matrix is: \n",
    "\\begin{array}{cc} \n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "\\end{array}\n",
    "After applying a margin of one on it, it will be:\n",
    "\\begin{array}{cc} \n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 1 & 1 & 0\\\\\n",
    "0 & 1 & 1 & 1 & 0\\\\\n",
    "0 & 1 & 1 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "\\end{array}\n",
    "After applying a margin of two on it, it will be:\n",
    "\\begin{array}{cc} \n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{array}\n",
    "\n",
    "The final margin added is the size of robot in pixel distance.\n",
    "\n",
    "Below figures shows the effect of adding margin,\n",
    "<table><tr>\n",
    "<td> <img src=\"Report_Figures/map1.png\" style='text-align: left' width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Map 1</b></p></td>\n",
    "<td><img src=\"Report_Figures/no_margin_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>No Margin</b></p></td>\n",
    "<td><img src=\"Report_Figures/add_margin_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Add Margin</b></p></td>\n",
    "</tr></table>\n",
    "<table><tr>\n",
    "<td> <img src=\"Report_Figures/map2.png\" style='text-align: left' width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Map 2</b></p></td>\n",
    "<td><img src=\"Report_Figures/no_margin_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>No Margin</b></p></td>\n",
    "<td><img src=\"Report_Figures/add_margin_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Add Margin</b></p></td>\n",
    "</tr></table>\n",
    "<p style='text-align: center;'><b>Figure 8: Addition of Margins to Obstacles</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pixel_to_distance(self):\n",
    "        # for get pixel_to_ditance value, use function \"get_pixel_to_distance\"\n",
    "        # aruco marker:71mmx71mm\n",
    "        # pixel_to_distance:return real world distance between two neighborhood pixels;unit:mm\n",
    "        pixel_to_distance = 71 / (self.side_length*2)  # unit:mm\n",
    "        \n",
    "        self.pixel_to_distance = pixel_to_distance\n",
    "\n",
    "        return self.pixel_to_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f745671",
   "metadata": {},
   "source": [
    "By knowing the real-world distance and pixel distance of aruco marker's side length, we can compute the conversion rate between pixel and real-world distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ddcaf",
   "metadata": {},
   "source": [
    "#### 3.1.4 Make it more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db545e",
   "metadata": {},
   "outputs": [],
   "source": [
    " def avg_globalmap(self,cap,avg_number=10):\n",
    "        #average first avg_number image to construct global map instead of only use one image\n",
    "        #in order to get rid of camera unstable problem\n",
    "        #return self.globalmap\n",
    "        weight = cap.get(3)\n",
    "        height=cap.get(4)\n",
    "        avg_map = np.zeros((int(weight), int(height)))\n",
    "        for i in range(avg_number):\n",
    "            ret, img = cap.read()  # returns ret and the frame\n",
    "            globalmap = self.compute_globalmap(img)\n",
    "            avg_map = np.add(globalmap, avg_map)\n",
    "        avg_map = np.divide(avg_map, avg_number)\n",
    "        avg_map[avg_map >= 0.5] = 1\n",
    "        avg_map[avg_map < 0.5] = 0\n",
    "        self.globalmap= avg_map\n",
    "        return self.globalmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ca58d2",
   "metadata": {},
   "source": [
    "When we tested our code, we found that only taking one frame from camera to construct the global map is unstable, because sometimes the frame taken is not in a good condition which result in generating a bad global map. Therefore, instead of only taking one frame, we take ten frames, and each frame computes one global map. After, we take the average of these ten global maps. Then, if the pixel is equal or larger than 0.5, which means that most of the frames consider this pixel as the obstacle, so we set the pixel to 1 (occupied). If the pixel is smaller than 0.5, which means that most of the frame consider this pixel as the free space, so we set the pixel to 0 (free)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd92c8",
   "metadata": {},
   "source": [
    "### 3.2 Compute vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vertex(globalmap):\n",
    "\n",
    "    globalmap = np.uint8(np.dot(np.transpose(globalmap), 255))\n",
    "    height,width=np.shape(globalmap)\n",
    "\n",
    "    # apply canny edge detection\n",
    "    edges = cv2.Canny(globalmap, 60, 160)\n",
    "\n",
    "    # apply morphology close\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    morph = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # get contours\n",
    "    contours = cv2.findContours(morph, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # get vertices\n",
    "    vertices = []\n",
    "    for i in range(len(contours[0])):\n",
    "        peri = cv2.arcLength(contours[0][i], True)\n",
    "        approx = cv2.approxPolyDP(contours[0][i], 0.01 * peri, True)\n",
    "        vertice = []\n",
    "        s=10 #margin for boarder of map\n",
    "        for i in range(len(approx)):\n",
    "            #push away the vertex at the border of map to make sure robot will not go to border\n",
    "            if approx[i][0][0]>(0+s):\n",
    "                if approx[i][0][0]<(width-s):\n",
    "                    if approx[i][0][1]>(0+s):\n",
    "                        if approx[i][0][1]<(height-s):pass\n",
    "                        else:approx[i][0][1]=approx[i][0][1]=10*height\n",
    "                    else:approx[i][0][1]=approx[i][0][1]=-10*height\n",
    "                else:approx[i][0][0]=approx[i][0][0]=10*width\n",
    "            else:approx[i][0][0]=approx[i][0][0]=-10*width\n",
    "            vertice.append(approx[i][0].tolist())\n",
    "        vertices.append(np.array(vertice))\n",
    "\n",
    "    return vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef36f4a6",
   "metadata": {},
   "source": [
    "To compute the vertixes of shapes in the image, we first find edges of each shape by Canny edge detection [3]. It first calculate the edge gradient and direction for every pixel. The direction of gradient which perpendicular to the edge will have a large magnitude because the change of pixel intensity is large at the edge. Then, in order to thin the edge, it perform non-maximum suppresion. For each pixel, it checked wheather it is the local maximum of its neighbourhood in its gradient direction. If it is, then keep this pixel, otherwise set pixel to 0. Then, using Hysteresis Thresholding to decide which are edge. We set one maximum value and one minimum value. If the intensity gradient is larger than maximum value, it is considered as edge. If the intensity gradient is smaller than minimum value, it is discarded. If the value is between maximum and minimum value, we check weather it is connected to the edge which is larger than maximum value. If yes, it is considered as edge, otherwise, it is discarded.\n",
    "\n",
    "Then, we finds countours of each shape. Since the contours are the boundary of shapes with same intensity, we can find countours by seraching each points [4]. If the point has pixel value 1 (1 is object), and its 4-neighbours or 8-neighbours contain pixel value 0 (0 is background), the point will be considered as the boundary. The algorithm will find both inner boundary and outer boundary. Here, we only use outer boundary. Also, to save memory, we only store the start and end point of each detected boundary, since all the boundaries of global obstacles are line. \n",
    "\n",
    "However, since the detected boundary is not so smooth, for example a rectangle, rather than only getting four corners, the ourput of countours also gave some unwanted points. Therefore, we use \"approxPolyDP\" to solve this problem. ApproxPolyDP function is an implementation of Douglas-Peucker algorithm [5] which can simplify the curve with fewer points. First, it considered the first point and last point are in the final curve. Then, it finds the farthest point from the line segment of first and last point. If the distance to farthest point is smaller than epsilon, the point is discarded. If the distance is larger then epsilon, the point are selected to the final curve. Then, recursively doing same process with start and the point selected, and then the last point and the point selected. Here, we set epsilon to the 0.01 of each contour's perimeter.\n",
    "\n",
    "Finally, the vertices of shapes are the points selected to the final curve.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Report_Figures/map1.png\" style='text-align: left' width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Map 1</b></p></td>\n",
    "<td><img src=\"Report_Figures/add_margin_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Add Margin</b></p></td>\n",
    "<td><img src=\"Report_Figures/edges_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Canny Edge</b></p></td>\n",
    "<td><img src=\"Report_Figures/contours_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Contours</b></p></td>\n",
    "<td><img src=\"Report_Figures/vertices_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Vertices</b></p></td>\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Report_Figures/map2.png\" style='text-align: left' width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Map 2</b></p></td>\n",
    "<td><img src=\"Report_Figures/add_margin_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Add Margin</b></p></td>\n",
    "<td><img src=\"Report_Figures/edges_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Canny Edge</b></p></td>\n",
    "<td><img src=\"Report_Figures/contours_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Contours</b></p></td>\n",
    "<td><img src=\"Report_Figures/vertices_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Vertices</b></p></td>\n",
    "</tr></table>\n",
    "\n",
    "<p style='text-align: center;'><b>Figure 9: Obstacle Treatment Step</b></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64317ea",
   "metadata": {},
   "source": [
    "### 3.3 Detect Robot Position and Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d98f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robot_position(image,robot_id=0):\n",
    "    #Detect robot position by aruco marker\n",
    "    #Return robot position; if not detected return None\n",
    "    #robot_id: robot aruco marker id\n",
    "\n",
    "    #Define aruco marker detector\n",
    "    arucoDict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_4X4_50)\n",
    "    arucoParams = cv2.aruco.DetectorParameters_create()\n",
    "\n",
    "    robot_position = None\n",
    "    angle=None\n",
    "\n",
    "    #Detect Aruco marker\n",
    "    (corners, ids, rejected) = cv2.aruco.detectMarkers(image, arucoDict,parameters=arucoParams)\n",
    "    if ids is not None:\n",
    "        for corner,number in zip(corners,ids):\n",
    "            (topLeft, topRight, bottomRight, bottomLeft) = corner[0]\n",
    "            centerpoint=(int((topLeft[0]+bottomRight[0])/2),int((topLeft[1]+bottomRight[1])/2))\n",
    "            if number==robot_id:\n",
    "                robot_position=centerpoint\n",
    "                angle=angle_calculate(topRight,topLeft)\n",
    "                break\n",
    "            else:pass\n",
    "    else:pass\n",
    "    return robot_position,angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be065d",
   "metadata": {},
   "source": [
    "We also use ArUco marker to detect the position of robot in real-time, since it's fast and robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d18269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_calculate(pt1,pt2):  \n",
    "    #Get robot angle\n",
    "    x=pt2[0]-pt1[0] \n",
    "    y=pt2[1]-pt1[1]\n",
    "    angle=np.arctan2(y,x)\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d856139d",
   "metadata": {},
   "source": [
    "Since each corner of marker is identified unequivocally, we can calculate the robot angle based on the angle of two corners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c2b92",
   "metadata": {},
   "source": [
    "## 4. Global Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873c78b",
   "metadata": {},
   "source": [
    "### 4.1 Compute Shortest Path\n",
    "\n",
    "As stated in the introduction, the goal of the global navigation is to compute the shortest path from the start to the goal using the image acquired by the camera. To do so, a library (PyVisGraph) is used. PyVisGraph first builds a visibility graph from the vertices of the obstacles and, then, uses Dijsktra's algorithm to find the shortest path.\n",
    "\n",
    "The coordinates of the start and goal as well as the coordinates of the vertices are given as output of the vision part. The vertices are formatted into an array which composition is as follows : Each row corresponds to a different obstacle and each element in the row corresponds to the coordinates (x & y) of the vertices of the obstacle.\n",
    "\n",
    "Knowing the vertices and the obstacles they form, it is possible to build a visibility graph. The latter determines which vertices are visible from each vertex of each obstacle. Having the coordinates of each vertex, the Euclidean norm of the segment linking two vertices can be used to determine their distance to each other. The problem of collision with obstacles is avoided by adding margins on the obstacles dimensions in the vision part directly.\n",
    "\n",
    "<img src=\"Report_Figures/Visibility_Graph.png\" style='text-align: center' width=\"550\" height=\"208;\"/><p style='text-align: center;'><b>Figure 10: Visibility Graph</b></p>\n",
    "\n",
    "<font color='red'>INCLUDE FIGURE VISIBILITY GRAPH</font>\n",
    "\n",
    "As mentioned earlier, Dijkstra's algorithm is used to compute the shortest path. Besides its easy implementation, this method has several advantages: It guarantees to find the shortest path and is relatively efficient (and can, therefore, be applied to fairly large problems). However, the path computed is not optimal (the robot could, sometimes, have a more direct path than traveling from vertex to vertex).\n",
    "\n",
    "The algorithm visits each vertex once beginning from the goal point to which it assigns a distance of zero. It computes the distance from the vertex it is currently visiting to each of the vertices preceding it (the vertices being linked to the current one in the visibility graph on the side of the starting point) and attributes them the distance computed. When the distance from the goal to a vertex is computed several times, only the shortest distance to it is attributed to this specific vertex. Once the distances are computed, the algorithm moves to the unvisited node with the lowest distance attached to it and repeats the process until reaching the starting point. \n",
    "\n",
    "Finally, the total distance from the starting point to the goal using the shortest path is the distance assigned to the starting point. As the algorithm keeps track of the shortest distance to each of the vertices at each stage, the shortest path can be found easily.\n",
    "\n",
    "<img src=\"Report_Figures/Dijkstra_Pseudo_Code.png\" style='text-align: center' width=\"550\" height=\"208;\"/><p style='text-align: center;'><b>Figure 11: Dijkstra's Pseudo Code</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e02ab",
   "metadata": {},
   "source": [
    "### 4.2 Follow Shortest Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8a668",
   "metadata": {},
   "source": [
    "Our global navigation control system uses a proportional controller to set the direction of the robot. Moreover, the robot speed is reduced to zero if the angle required for rotation is too big, else it is 100mm/s. \n",
    "\n",
    "<img \n",
    "src=\"Report_Figures/schema_speeds.png\" style='text-align: center' width=\"550\" height=\"208;\"/><p style='text-align: center;'><b>Figure 12: Robot rotation schema</b></p>\n",
    "\n",
    "The robot speed is the mean of the speed of the left motor and the right motor.\n",
    "The angle is set by the difference of speed of the left/right motor. It isn’t modified by the offset speed as you can see in these equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe3160",
   "metadata": {},
   "source": [
    "$Speed_{left}=\\delta angle \\cdot Radius \\\\\n",
    "Speed_{right}=\\delta angle \\cdot (Radius+Diameter) \\\\\n",
    "\\delta angle= \\frac{Speed_{right}-Speed_{left}}{Diameter}\\\\\n",
    "Mean_{speed}=\\delta angle \\cdot (Radius+\\frac{Diameter}{2})=\\frac{Speed_{left}+Speed_{right}}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7201b386",
   "metadata": {},
   "source": [
    "It permits us to control both of the direction and speed of the robot separately.\n",
    "\n",
    "To follow the path, we just have a global variable which set the position in the path (list of points) so that we have a point goal. Every iteration we study the distances between points on the paths to eventually change goal. If we end the list, we arrived.\n",
    "\n",
    "We add a coefficient of 1.5 to the motors target to have the right speed. We saw that this coefficient is dependent of the speed, but we approximate to 1.5 for simplicity. Also, we bounded the speeds to +-400mm/s. \n",
    "Then the speeds are transmit to the navigation() function, which control the motors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51153aed",
   "metadata": {},
   "source": [
    "### 4.3 Choice of Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b2699",
   "metadata": {},
   "source": [
    "We didn’t add some integration (PI) because we founded the system smooth already. We added some of the simulations that we made to design the controller. Even if there is not noise and error in displacement it gives an idea of time of reaction and smoothness of the motors when we change path point. We see that if we want to minimize the error (distance between the robot and the path) we have to set a strong controller which correct fast. This implies a big variation of the speeds of the motors, the inputs of the system. With a smooth correction we don’t achieve to avoid large pics as you can see in these pictures. \n",
    "\n",
    "We also tried to adapt the speed to the angle and derivate speed, as you can see in the second example.\n",
    "\n",
    "One can note that here we simulated as 1 second long loop. It is a margin for the time taken to obtain a picture by the camera. Now we know that this time is very over evaluated but the simulations are still pertinent.\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"Report_Figures/map_PID_50.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>map</b></p></td>\n",
    "<td><img src=\"Report_Figures/speed_PID_50.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>speeds</b></p></td>\n",
    "</tr></table>\n",
    "<p style='text-align: center;'><b>50mm/s speed, PID controller (still improvable) evaluated 1 time/second</b></p>\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"Report_Figures/map_PID_100.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>map</b></p></td>\n",
    "<td><img src=\"Report_Figures/speed_PID_100.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>speeds</b></p></td>\n",
    "</tr></table>\n",
    "<p style='text-align: center;'><b>100mm/s speed, PID and speed adaptation evaluated 1 time/second</b></p>\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"Report_Figures/map_P_100.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>map</b></p></td>\n",
    "<td><img src=\"Report_Figures/speed_P_100.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>speeds</b></p></td>\n",
    "</tr></table>\n",
    "<p style='text-align: center;'><b>100mm/s speed, P controller evaluated 1 time/second</b></p>\n",
    "\n",
    "100mm/s speed P evaluated 1 time/second\n",
    "We see that obviously the P is less accurate but the pic in the PID show that we can’t avoid some hard reactions if we want to rest in the path. \n",
    "To conclude we just made a Proportional controller because without speaking about the computational inclusion of derivative and integrative calculation it wasn't optimal to precisely follow the path in our case. By observation we also are satisfied about the fluidity of the robot displacements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915eb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_nav(path):\n",
    "    global state,index,direction,speed_left,speed_right\n",
    "    \n",
    "    #constants\n",
    "    v_max=400#mm/s\n",
    "    Diameter=94#mm\n",
    "    goal_margin=20#mm\n",
    "    threshold=0.3#rad\n",
    "    Kp=0.8\n",
    "    \n",
    "    #CHECK GOAL\n",
    "    if math.sqrt((robot_state[0]-path[index][0])**2+(robot_state[1]-path[index][1])**2)<goal_margin:\n",
    "        if index+1>=int(np.size(path)/2):\n",
    "            print('arrived')\n",
    "            state=2\n",
    "            return 0,0\n",
    "        else:\n",
    "            index=index+1\n",
    "            print('index : ',index)\n",
    "    \n",
    "    #MOTORS\n",
    "    #calculate angle correction\n",
    "    obj_angle=np.arctan2(path[index][1]-robot_state[1],path[index][0]-robot_state[0])\n",
    "    delta_angle=obj_angle-direction\n",
    "    while(delta_angle>np.pi):\n",
    "        delta_angle=obj_angle-np.pi\n",
    "    while(delta_angle<=-np.pi):\n",
    "        delta_angle=obj_angle+np.pi\n",
    "        \n",
    "    #mean speed\n",
    "    if abs(delta_angle)>threshold:\n",
    "        vitesse=0\n",
    "    else:\n",
    "        vitesse=100\n",
    "    \n",
    "    #speed_left/right according to angle correction\n",
    "    speed_left=(Kp*(delta_angle))*Diameter\n",
    "    speed_right=-(Kp*(delta_angle))*Diameter\n",
    "    \n",
    "    #bounded speed for turn\n",
    "    if speed_right<-v_max:speed_right=-v_max\n",
    "    if speed_right>v_max:speed_right=v_max\n",
    "    if speed_left<-v_max:speed_left=-v_max\n",
    "    if speed_left>v_max:speed_left=v_max\n",
    "    \n",
    "    #bounded speed for turn+speedDC\n",
    "    delta_speed=abs(speed_left-speed_right)\n",
    "    if speed_left+vitesse>v_max:#PB si les deux au dessus\n",
    "        speed_left=v_max\n",
    "        speed_right=v_max-delta_speed\n",
    "    elif speed_right+vitesse>v_max:\n",
    "        speed_left=v_max-delta_speed\n",
    "        speed_right=v_max\n",
    "    else:\n",
    "        speed_left=speed_left+vitesse\n",
    "        speed_right=speed_right+vitesse\n",
    "    \n",
    "    return speed_left,speed_right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d2bc8",
   "metadata": {},
   "source": [
    "## 5. Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22404e18",
   "metadata": {},
   "source": [
    "The Kalman function is dedicated to fusing and filtering the output of two sensors (the camera and the sensors on the motors) in order to improve the precision of the localization of the thymio. The Kalman filter is implemented using the correponsding functions from the filterpy library. To do so, we determine 5 matrixes that describe our system:\n",
    "\n",
    "-\tThe matrix F describes the relation between the current state and the next state. \n",
    "-\tThe matrix H describes the relation between the state and the measurements\n",
    "-\tThe matrix P is a state covariance matrix. It is a diagonal matrix which elements are the variance of each state.\n",
    "-\tThe matrix P gives information about the noise of each measurement. It is also a diagonal matrix, which elements are the variance of each measured value.\n",
    "-\tThe matrix Q is related to the process noise (which we decided to set as discrete white noise).\n",
    "\n",
    "The next step consists in using the different sensors to obtain measurements.\n",
    "\n",
    "Two functions are used to do a prediction about a state, to update the measurements and to filter the state. The function output is the filtered state.\n",
    "\n",
    "Our state robot_state has four components: x and y position and x and y speed ($v_x$ and $v_y$). Our measurements also have four components: x and y position measurements from the camera that is updated in each iteration of the main loop (as a new image is being taken) as well as $v_x$ and $v_y$ measured directly on the motors. \n",
    "\n",
    "The motor speeds are given by left and right speed. We use the direction of the robot which is a global variable, to transform them into x and y speeds. \n",
    "\n",
    "### 5.1 Hidden camera scenario\n",
    "\n",
    "We wanted our Kalman to still update the state of the robot correctly even when the camera is temporarily hidden. To do so, we add a condition to see if a new image has been taken. If no new image is available, the Kalman doesn’t use the position measurement from the camera which is outdated and would false the filtering. Instead, we use the current x and y position of the robot from robot_state instead of the position measurement as a temporary measure. Once the camera is back on, the position measurements from the camera will be used again. \n",
    "\n",
    "### 5.2 Kalman parameters \n",
    "\n",
    "The variances of our state position and speed were tuned by trial and error during the testing of the Kalman filter with the rest of the code.\n",
    "\n",
    "The variance of the measurements, both from the camera and the motors were determined experimentally. We implemented a python code that records 1000 measurements from either the camera or the motor sensors and calculates the variance. Those measurements were made while the Thymio kept either the same position or the same speed, depending on the value measured by the sensor. The variance found varied each time we run our code. So we dediced to take the mean of the several resulting variances.\n",
    "\n",
    "<font color='red'>REMARK : BETTER EXPLAIN 1000 MEASUREMENTS</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973555ab",
   "metadata": {},
   "source": [
    "## 6. Motor Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3fc49d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a3c983",
   "metadata": {},
   "source": [
    "## 7. Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e4b82",
   "metadata": {},
   "source": [
    "Navigation is the function that controls the state of the robot and applies the corresponding speed on the two motors. It is called once in each iteration of the main loop. There are two states: state 0 and state 1.\n",
    "\n",
    "In state 0, the robot is in global navigation mode and follows the global_path trajectory given as input to the navigation function. When in state 0, the global_nav() function is called with the path as input. The speeds given by global_nav() are then applied to the motors.\n",
    "\n",
    "<font color='red'>CAREFUL ABOUT SEPARATION BETWEEN MOTOR CONTROL AND LOCAL NAVIGATION</font>\n",
    "\n",
    "In state 1, the robot is avoiding a 3D obstacle that was added and, therefore, not recorded on the global map. Each time navigation() is called, the prox.horizontal values of the thymio are read. We compute the normalized weighted sum of the two sensor values on the left (prox[0] and prox[1]) on the one hand and of the two right sensor values (prox[3] and prox[4]). The sum is weighted (weight of 1/2 for the proximity sensor on the far right/center right) to give more importance on the measurements given by sensors closer to the central axis of the robot. Indeed, an obstacle being closer to this axis will require a higher deviation from the initial path to avoid it.\n",
    "\n",
    "The velocity of the left (right) motor is computed by subtracting the result of the weighted sum of measurements given by the left (right respectively) proximity sensors multiplied by a coefficient and subtracted to a constant value. This way, the left prox sum is contributing to the left motor speed and the right prox sum to the right one. In turn, the robot turns in the opposite direction of the detected obstacle.\n",
    "\n",
    "The switching between the different states is done with the measured values of the proximity sensors. If at least one of the four exterior sensor values is above a certain threshold, the robot switches to state 1. If all the proximity values are below another set threshold, the state switches back to 0. \n",
    "\n",
    "We experienced problems when merging the global and local navigation. Depending on the geometry and the placement of the 3d obstacle in relation to the global path, the robot would sometimes get stuck. As a matter of fact, it was trying to avoid the obstacle but, as soon as the prox value was below the threshold, it would turn back towards the obstacle when trying to follow the global path. This meant the thymio was constantly changing state and switching from two opposite objectives: turning one direction to avoid the obstacle and turning the opposite direction to reach its goal. \n",
    "\n",
    "In order to tackle this error, when the robot switches back to state 0 from state 1, meaning when the obstacle is no longer detected, the global path towards the goal is recomputed with a starting point in front of the thymio. As a result, when the robot turned enough to avoid the obstacle, it goes straight to a new starting point and follows its new trajectory towards the goal, this time from a place far enough from the obstacle. In turn, the path given as input to the global_nav() function when the robot is in state 0 is either the original shortest path calculated during the initialization outside the main loop, or the new path calculated when moving away from a 3d obstacle. \n",
    "\n",
    "<font color='red'>INCLUDE FIGURE LOCAL NAVIGATION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24221efb",
   "metadata": {},
   "source": [
    "## 8. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ea083",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a2cf36",
   "metadata": {},
   "source": [
    "## 9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37dd8e",
   "metadata": {},
   "source": [
    "[1] OpenCV. <i>Image Thresholding</i>. https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html\n",
    "\n",
    "[2] OpenCV. <i>Detection of ArUco Markers</i>. https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html\n",
    "\n",
    "[3] OpenCV. <i>Canny Edge Dectecion</i>. https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html\n",
    "\n",
    "[4] OpenCV. <i>Contours : Getting Started</i>. https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html\n",
    "\n",
    "[5] https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm\n",
    "\n",
    "[6] https://github.com/TaipanRex/pyvisgraph\n",
    "\n",
    "[7] Basics of Mobile Robotics\n",
    "\n",
    "[8] http://cs.indstate.edu/~rjaliparthive/dijkstras.pdf\n",
    "\n",
    "[9] https://courses.lumenlearning.com/waymakermath4libarts/chapter/shortest-path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136609a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
