{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3455097",
   "metadata": {},
   "source": [
    "<h1><center>Project</center></h1>\n",
    "<h2><center>Micro-452 : Basics of Mobile Robotics</center></h2>\n",
    "<h3><center>Group 11</center></h3>\n",
    "<h3><center>Benoist Marguerite, Chang Chun-Tzu, Maradan Théodore, Tambourin Noé</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8fc8b",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This report presents the project done in the Basics of Mobile Robotics class. The project consists in programming a robot (the \"Thymio\") to navigate in an environment containing a set of (expected and unexpected) obstacles.\n",
    "\n",
    "To do so, the robot should first be able to build a global map of its environment using a camera (<b>vision</b>). From the map, the robot should determine the shortest path from its starting point to its goal (<b>global naviagation</b>). Then, <b>filtering</b> the information coming from the camera and from the built-in sensors the robot should travel to a goal point set anywhere on the map using <b>motor control</b>. Finally, the robot should avoid any unexpected obstacle on its path using <b>local navigation</b>.\n",
    "\n",
    "<font color='red'>INCLUDE DIAGRAM PROJECT</font>\n",
    "\n",
    "The sections below describe in detail our solution to the problem and, in particular, the choices and assumptions made through the realization of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c627765",
   "metadata": {},
   "source": [
    "## 2. Environment Choice\n",
    "\n",
    "The environment constituting the global map was chosen to be blank with 2D black obstacles (obstacles without thickness). The contrast between the map and the obstacles allows for a better detection of the obstacles by the camera. In contrast, the unexpected obstacles are 3D (non-null thickness).\n",
    "\n",
    "This distinction ensures that the obstacles detected by the proximity sensors are unexpected obstacles and avoids confusion with obstacles being part of the global map.\n",
    "\n",
    "<font color='red'>INCLUDE FIGURE ENVIRONMENT</font>\n",
    "\n",
    "In order to identify the robot and the goal point, ArUco markers are used. These synthetic markers are easy to identify using an appropriate library (OpenCV). Moreover, they ensure a reliable robot pose acquisition by the camera.\n",
    "\n",
    "<font color='red'>INCLUDE FIGURE ARUCO MARKER</font>\n",
    "\n",
    "Finally, the camera is fixed above the global map so that it sees the entire environment while avoiding as much as possible distortions if the map due to the camera not being parallel with the plane of the map.\n",
    "\n",
    "<font color='red'>INCLUDE FIGURE ENVIRONMENT + CAMERA</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b22a8b",
   "metadata": {},
   "source": [
    "## 3. Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd1415",
   "metadata": {},
   "source": [
    "The first step of the project is to construct the map with the vision. We fisrt use Otsu's threshold algorithm to recognized the black global obstacles and then use aruco marker to detect goal point and start point which is where robot is at the beginning.\n",
    "\n",
    "### 3.1 Otsu's threshold method\n",
    "\n",
    "Image thresholding binarizes a grayscale image based on pixel values and a threshold value. For example, when the pixel value is samller than the threshold value, this pixel will be marked as black. When the pixel value is larger then the threshold value, this pixel will be marked as white. The threshold algorithm we choosed is Otsu's threshold. Comparing to simple threshold method which use an manually choosen value as threshold, Otsu's threshold method [1] tries to find a global optimal threshold value from image histogram which can minimize the weighted within-class variance given by:\n",
    "\n",
    "$\\sigma_w^2(t) = q_1(t)\\sigma_1^2(t)+q_2(t)\\sigma_2^2(t)\\$\n",
    "\n",
    "where\n",
    "\n",
    "$q_1(t) = \\sum_{i=1}^{t} P(i) \\quad \\& \\quad q_2(t) = \\sum_{i=t+1}^{I} P(i)$\n",
    "\n",
    "$\\mu_1(t) = \\sum_{i=1}^{t} \\frac{iP(i)}{q_1(t)} \\quad \\& \\quad \\mu_2(t) = \\sum_{i=t+1}^{I} \\frac{iP(i)}{q_2(t)}$\n",
    "\n",
    "$\\sigma_1^2(t) = \\sum_{i=1}^{t} [i-\\mu_1(t)]^2 \\frac{P(i)}{q_1(t)} \\quad \\& \\quad \\sigma_2^2(t) = \\sum_{i=t+1}^{I} [i-\\mu_2(t)]^2 \\frac{P(i)}{q_2(t)}$\n",
    "\n",
    "In our map, we have black (global obstacle) and white (free move space) pixels, so the image histogram will have two peaks. Then, Otsu's threshold method find a value lying between two peaks to minimizes both classes variances which is the optimal threshold value we used. By using Otsu's threshold, it will help us to choose the optimal threshold value rather than manually choose the threshold value evrytime depending on the environment when using simple threshold method. Following figures show the differnce between using two methods with differnt environment,\n",
    "<table><tr>\n",
    "<td> <img src=\"report_image/map1.png\" style='text-align: left' width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Map 1</b></p></td>\n",
    "<td><img src=\"report_image/Otsus_thresholding_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Otsu's threshold</b></p></td>\n",
    "<td><img src=\"report_image/simple_threshold_method_50_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Simple threshold, threshold=50</b></p></td>\n",
    "<td><img src=\"report_image/simple_threshold_method_127_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Simple threshold, threshold=127</b></p></td>\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"report_image/map2.png\" style='text-align: left' width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Map 2</b></p></td>\n",
    "<td><img src=\"report_image/Otsus_thresholding_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Otsu's threshold</b></p></td>\n",
    "<td><img src=\"report_image/simple_threshold_method_50_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Simple threshold, threshold=50</b></p></td>\n",
    "<td><img src=\"report_image/simple_threshold_method_127_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Simple threshold, threshold=127</b></p></td>\n",
    "</tr></table>\n",
    "\n",
    "We can observe that Otsu's threshold shows stable performance with differnt evironment. Simple threshold method performs better with threshold value 127 in Map 1, however, in Map 2, threshold value 50 perfomrs better. Therefore, using simple threshold method may need to manually adjust the threshold value when the environment changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88242af7",
   "metadata": {},
   "source": [
    "### 3.2 Aruco Marker\n",
    "\n",
    "Then, we need to detect goal point and start point which is the robot position for constructing global map. To detect the position of objects, we use aruco marker which is fast and robust.\n",
    "\n",
    "Aruco marker [2] provides the position of its four conners and uses its inner binary matrix to determine its id. To detect an aruco marker, it fisrt apllies adaptive threshold and then extractes the contours of shapes in the image. If the extracted shape is not a convex or not simillar to a square shape, it will be discarded. After selecting the candidates of marker, it need to extracte the inner bits of each candidates. Otsu's thresholding  is used to first seperate black and white pixels in the image. Then, the extracted shape is divided to differnt cell according to the size of marker and boarder. The cell which contains more black pixels will be considered as a black bit, vice versa. Finally, we can get four coners and id by binary codification.\n",
    "\n",
    "Below shows some example of aruco marker detection.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"report_image/arucomarker_map1.png\" style='text-align: left' width=\"320\" height=\"416;\"/><p style='text-align: center;'><b>Map 1</b></p></td>\n",
    "<td> <img src=\"report_image/arucomarker_map2.png\" style='text-align: left' width=\"320\" height=\"416;\"/><p style='text-align: center;'><b>Map 2</b></p></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68654b31",
   "metadata": {},
   "source": [
    "### 3.3 Add margin\n",
    "\n",
    "Since the robot is not only a point, we need to add margin to the global obstacles to insure that every part of robot will not go into it. Here, we use binary dilation to increase the area of obstacles with the structure we defined. We defined the increasing margin is a 3x3 strcuting element with connectivity 2, which is:\n",
    " \\begin{array}{cc} \n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 1\\\\\n",
    "\\end{array}\n",
    "If the original matrix is: \n",
    "\\begin{array}{cc} \n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "\\end{array}\n",
    "After applying one margin on it, it will be:\n",
    "\\begin{array}{cc} \n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 1 & 1 & 0\\\\\n",
    "0 & 1 & 1 & 1 & 0\\\\\n",
    "0 & 1 & 1 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "\\end{array}\n",
    "After applying two margin on it, it will be:\n",
    "\\begin{array}{cc} \n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{array}\n",
    "\n",
    "The number of margin added is the size of robot in pixel distance.\n",
    "\n",
    "Below figures shows the effect of adding margin,\n",
    "<table><tr>\n",
    "<td> <img src=\"report_image/map1.png\" style='text-align: left' width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Map 1</b></p></td>\n",
    "<td><img src=\"report_image/no_margin_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>No Margin, threshold=50</b></p></td>\n",
    "<td><img src=\"report_image/add_margin_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Add Margin</b></p></td>\n",
    "</tr></table>\n",
    "<table><tr>\n",
    "<td> <img src=\"report_image/map2.png\" style='text-align: left' width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Map 2</b></p></td>\n",
    "<td><img src=\"report_image/no_margin_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>No Margin, threshold=50</b></p></td>\n",
    "<td><img src=\"report_image/add_margin_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Add Margin</b></p></td>\n",
    "</tr></table>\n",
    "\n",
    "By knowing the real-world distance and pixel distance of aruco marker's side length, we can compute the conversion rate between pixel and real-world distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ddcaf",
   "metadata": {},
   "source": [
    "### 3.4 Make it more robust\n",
    "\n",
    "When we tested our code, we found that only taking one frame from camera to construct the global map is unstable, because sometimes the frame taken is not in a good condition which result in generating a bad global map. Therefore, instead of only taking one frame, we take ten frames, and each frame computes one global map. After, we take the average of these ten global maps. Then, if the pixel is equal or larger than 0.5, which means that most of the frames consider this pixel as the obstacle, so we set the pixel to 1 (occupied). If the pixel is smaller than 0.5, which means that most of the frame consider this pixel as the free space, so we set the pixel to 0 (free)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64317ea",
   "metadata": {},
   "source": [
    "### 3.5 Detect Robot Position and Angle\n",
    "\n",
    "We also use aruco marker to detect the position of robot in real-time, since it's fast and robust.\n",
    "\n",
    "Since each corner of marker is identified unequivocally, we can calculate the robot angle based on the angle of two corners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf2b56c",
   "metadata": {},
   "source": [
    "### 3.6 Compute vertices\n",
    "\n",
    "To compute the vertixes of shapes in the image, we first find edges of each shape by Canny edge detection [3]. It first calculate the edge gradient and direction for every pixel. The direction of gradient which perpendicular to the edge will have a large magnitude because the change of pixel intensity is large at the edge. Then, in order to thin the edge, it perform non-maximum suppresion. For each pixel, it checked wheather it is the local maximum of its neighbourhood in its gradient direction. If it is, then keep this pixel, otherwise set pixel to 0. Then, using Hysteresis Thresholding to decide which are edge. We set one maximum value and one minimum value. If the intensity gradient is larger than maximum value, it is considered as edge. If the intensity gradient is smaller than minimum value, it is discarded. If the value is between maximum and minimum value, we check weather it is connected to the edge which is larger than maximum value. If yes, it is considered as edge, otherwise, it is discarded.\n",
    "\n",
    "Then, we finds countours of each shape. Since the contours are the boundary of shapes with same intensity, we can find countours by seraching each points [4]. If the point has pixel value 1 (1 is object), and its 4-neighbours or 8-neighbours contain pixel value 0 (0 is background), the point will be considered as the boundary. The algorithm will find both inner boundary and outer boundary. Here, we only use outer boundary. Also, to save memory, we only store the start and end point of each detected boundary, since all the boundaries of global obstacles are line. \n",
    "\n",
    "However, since the detected boundary is not so smooth, for example a rectangle, rather than only getting four corners, the ourput of countours also gave some unwanted points. Therefore, we use \"approxPolyDP\" to solve this problem. ApproxPolyDP function is an implementation of Douglas-Peucker algorithm [5] which can simplify the curve with fewer points. First, it considered the first point and last point are in the final curve. Then, it finds the farthest point from the line segment of first and last point. If the distance to farthest point is smaller than epsilon (\"peri\" in the code), the point is discarded. If the distance is larger then epsilon, the point are selected to the final curve. Then, recursively doing same process with start and the point selected, and then the last point and the point selected.\n",
    "\n",
    "Finally, the vertices of shapes are the points selected to the final curve.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"report_image/map1.png\" style='text-align: left' width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Map 1</b></p></td>\n",
    "<td><img src=\"report_image/add_margin_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Add Margin</b></p></td>\n",
    "<td><img src=\"report_image/edges_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Canny Edge</b></p></td>\n",
    "<td><img src=\"report_image/contours_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Contours</b></p></td>\n",
    "<td><img src=\"report_image/vertices_map1.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Vertices</b></p></td>\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"report_image/map2.png\" style='text-align: left' width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Map 2</b></p></td>\n",
    "<td><img src=\"report_image/add_margin_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Add Margin</b></p></td>\n",
    "<td><img src=\"report_image/edges_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Canny Edge</b></p></td>\n",
    "<td><img src=\"report_image/contours_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Contours</b></p></td>\n",
    "<td><img src=\"report_image/vertices_map2.png\" width=\"160\" height=\"208;\"/><p style='text-align: center;'><b>Vertices</b></p></td>\n",
    "</tr></table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c2b92",
   "metadata": {},
   "source": [
    "## 4. Global Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873c78b",
   "metadata": {},
   "source": [
    "As stated in the introduction, the goal of the global navigation is to compute the shortest path from the start to the goal using the image acquired by the camera. To do so, a library (PyVisGraph) is used. PyVisGraph first builds a visibility graph from the vertices of the obstacles and, then, uses Dijsktra's algorithm to find the shortest path.\n",
    "\n",
    "The coordinates of the start and goal as well as the coordinates of the vertices are given as output of the vision part. The vertices are formatted into an array which composition is as follows : Each row corresponds to a different obstacle and each element in the row corresponds to the coordinates (x & y) of the vertices of the obstacle.\n",
    "\n",
    "Knowing the vertices and the obstacles they form, it is possible to build a visibility graph. The latter determines which vertices are visible from each vertex of each obstacle. Having the coordinates of each vertex, the Euclidean norm of the segment linking two vertices can be used to determine their distance to each other. The problem of collision with obstacles is avoided by adding margins on the obstacles dimensions in the vision part directly.\n",
    "\n",
    "<font color='red'>INCLUDE FIGURE VISIBILITY GRAPH</font>\n",
    "\n",
    "As mentioned earlier, Dijkstra's algorithm is used to compute the shortest path. Besides its easy implementation, this method has several advantages: It guarantees to find the shortest path and is relatively efficient (and can, therefore, be applied to fairly large problems). However, the path computed is not optimal (the robot could, sometimes, have a more direct path than traveling from vertex to vertex).\n",
    "\n",
    "The algorithm visits each vertex once beginning from the goal point to which it assigns a distance of zero. It computes the distance from the vertex it is currently visiting to each of the vertices preceding it (the vertices being linked to the current one in the visibility graph on the side of the starting point) and attributes them the distance computed. When the distance from the goal to a vertex is computed several times, only the shortest distance to it is attributed to this specific vertex. Once the distances are computed, the algorithm moves to the unvisited node with the lowest distance attached to it and repeats the process until reaching the starting point. \n",
    "\n",
    "Finally, the total distance from the starting point to the goal using the shortest path is the distance assigned to the starting point. As the algorithm keeps track of the shortest distance to each of the vertices at each stage, the shortest path can be found easily.\n",
    "\n",
    "<font color='red'>INCLUDE PSEUDO CODE DIJKSTRA</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d2bc8",
   "metadata": {},
   "source": [
    "## 5. Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22404e18",
   "metadata": {},
   "source": [
    "The Kalman function is dedicated to fusing and filtering the output of two sensors (the camera and the sensors on the motors) in order to improve the precision of the localization of the thymio. The Kalman filter is implemented using the correponsding functions from the filterpy library. To do so, we determine 5 matrixes that describe our system:\n",
    "\n",
    "-\tThe matrix F describes the relation between the current state and the next state. \n",
    "-\tThe matrix H describes the relation between the state and the measurements\n",
    "-\tThe matrix P is a state covariance matrix. It is a diagonal matrix which elements are the variance of each state.\n",
    "-\tThe matrix P gives information about the noise of each measurement. It is also a diagonal matrix, which elements are the variance of each measured value.\n",
    "-\tThe matrix Q is related to the process noise (which we decided to set as discrete white noise).\n",
    "\n",
    "The next step consists in using the different sensors to obtain measurements.\n",
    "\n",
    "Two functions are used to do a prediction about a state, to update the measurements and to filter the state. The function output is the filtered state.\n",
    "\n",
    "Our state robot_state has four components: x and y position and x and y speed ($v_x$ and $v_y$). Our measurements also have four components: x and y position measurements from the camera that is updated in each iteration of the main loop (as a new image is being taken) as well as $v_x$ and $v_y$ measured directly on the motors. \n",
    "\n",
    "The motor speeds are given by left and right speed. We use the direction of the robot which is a global variable, to transform them into x and y speeds. \n",
    "\n",
    "### 5.1 Hidden camera scenario\n",
    "\n",
    "We wanted our Kalman to still update the state of the robot correctly even when the camera is temporarily hidden. To do so, we add a condition to see if a new image has been taken. If no new image is available, the Kalman doesn’t use the position measurement from the camera which is outdated and would false the filtering. Instead, we use the current x and y position of the robot from robot_state instead of the position measurement as a temporary measure. Once the camera is back on, the position measurements from the camera will be used again. \n",
    "\n",
    "### 5.2 Kalman parameters \n",
    "\n",
    "The variances of our state position and speed were tuned by trial and error during the testing of the Kalman filter with the rest of the code.\n",
    "\n",
    "The variance of the measurements, both from the camera and the motors were determined experimentally. We implemented a python code that records 1000 measurements from either the camera or the motor sensors and calculates the variance. Those measurements were made while the Thymio kept either the same position or the same speed, depending on the value measured by the sensor. The variance found varied each time we run our code. So we dediced to take the mean of the several resulting variances.\n",
    "\n",
    "<font color='red'>REMARK : BETTER EXPLAIN 1000 MEASUREMENTS</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973555ab",
   "metadata": {},
   "source": [
    "## 6. Motor Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3fc49d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a3c983",
   "metadata": {},
   "source": [
    "## 7. Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e4b82",
   "metadata": {},
   "source": [
    "Navigation is the function that controls the state of the robot and applies the corresponding speed on the two motors. It is called once in each iteration of the main loop. There are two states: state 0 and state 1.\n",
    "\n",
    "In state 0, the robot is in global navigation mode and follows the global_path trajectory given as input to the navigation function. When in state 0, the global_nav() function is called with the path as input. The speeds given by global_nav() are then applied to the motors.\n",
    "\n",
    "<font color='red'>CAREFUL ABOUT SEPARATION BETWEEN MOTOR CONTROL AND LOCAL NAVIGATION</font>\n",
    "\n",
    "In state 1, the robot is avoiding a 3D obstacle that was added and, therefore, not recorded on the global map. Each time navigation() is called, the prox.horizontal values of the thymio are read. We compute the normalized weighted sum of the two sensor values on the left (prox[0] and prox[1]) on the one hand and of the two right sensor values (prox[3] and prox[4]). The sum is weighted (weight of 1/2 for the proximity sensor on the far right/center right) to give more importance on the measurements given by sensors closer to the central axis of the robot. Indeed, an obstacle being closer to this axis will require a higher deviation from the initial path to avoid it.\n",
    "\n",
    "The velocity of the left (right) motor is computed by subtracting the result of the weighted sum of measurements given by the left (right respectively) proximity sensors multiplied by a coefficient and subtracted to a constant value. This way, the left prox sum is contributing to the left motor speed and the right prox sum to the right one. In turn, the robot turns in the opposite direction of the detected obstacle.\n",
    "\n",
    "The switching between the different states is done with the measured values of the proximity sensors. If at least one of the four exterior sensor values is above a certain threshold, the robot switches to state 1. If all the proximity values are below another set threshold, the state switches back to 0. \n",
    "\n",
    "We experienced problems when merging the global and local navigation. Depending on the geometry and the placement of the 3d obstacle in relation to the global path, the robot would sometimes get stuck. As a matter of fact, it was trying to avoid the obstacle but, as soon as the prox value was below the threshold, it would turn back towards the obstacle when trying to follow the global path. This meant the thymio was constantly changing state and switching from two opposite objectives: turning one direction to avoid the obstacle and turning the opposite direction to reach its goal. \n",
    "\n",
    "In order to tackle this error, when the robot switches back to state 0 from state 1, meaning when the obstacle is no longer detected, the global path towards the goal is recomputed with a starting point in front of the thymio. As a result, when the robot turned enough to avoid the obstacle, it goes straight to a new starting point and follows its new trajectory towards the goal, this time from a place far enough from the obstacle. In turn, the path given as input to the global_nav() function when the robot is in state 0 is either the original shortest path calculated during the initialization outside the main loop, or the new path calculated when moving away from a 3d obstacle. \n",
    "\n",
    "<font color='red'>INCLUDE FIGURE LOCAL NAVIGATION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24221efb",
   "metadata": {},
   "source": [
    "## 8. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ea083",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a2cf36",
   "metadata": {},
   "source": [
    "## 9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37dd8e",
   "metadata": {},
   "source": [
    "- https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html\n",
    "- https://github.com/TaipanRex/pyvisgraph\n",
    "- Basics of Mobile Robotics\n",
    "- http://cs.indstate.edu/~rjaliparthive/dijkstras.pdf\n",
    "- https://courses.lumenlearning.com/waymakermath4libarts/chapter/shortest-path/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
